# MiniAlphaGo for Reversi

这是一个基于蒙特卡洛树搜索（MCTS）和深度学习实现的迷你版AlphaGo，用于玩反转棋（黑白棋/Othello）游戏。

## 功能特点

- 完整的反转棋游戏实现
- 基于神经网络增强的蒙特卡洛树搜索（MCYTS）算法
- 使用PyQt5开发的图形用户界面
- 支持自我对弈训练神经网络
- 处理反转棋中的特殊规则（如跳过回合）

## 安装

1. 安装Python 3.8或更高版本
2. 安装依赖：
```bash
pip install -r requirements.txt
```

## 使用方法

### 运行图形界面进行游戏
```bash
python gui.py
```

### 训练神经网络
```bash
python train.py
```
训练过程会生成一个`models`文件夹，其中包含训练好的策略网络和价值网络模型。

## 项目结构

- `reversi.py`: 游戏核心逻辑，包括棋盘状态、规则实现和胜负判定
- `mcts.py`: MCYTS算法实现，结合神经网络进行决策
- `neural_network.py`: 神经网络模型，包括策略网络和价值网络
- `gui.py`: 图形用户界面
- `train.py`: 神经网络训练模块，通过自我对弈生成训练数据
- `requirements.txt`: 项目依赖列表

## 工作原理

1. **游戏规则**：反转棋在8x8棋盘上进行，玩家轮流放置棋子。当一个棋子放置后，它会翻转所有被夹在其与另一个同色棋子之间的对手棋子。

2. **MCYTS算法**：
   - 选择：从根节点开始，使用UCB公式选择子节点
   - 扩展：创建新的子节点
   - 模拟：使用策略网络和价值网络评估局面
   - 反向传播：将评估结果反向传播

3. **神经网络**：
   - 策略网络：预测每个位置的落子概率
   - 价值网络：评估当前局面的胜率

4. **训练过程**：
   - 自我对弈生成训练数据
   - 使用生成的数据训练神经网络
   - 重复上述过程，不断改进AI性能

## 自定义

- 修改`gui.py`中的`self.human_player`可以切换玩家颜色
- 在`mcts.py`中调整`num_simulations`参数可以改变AI的思考深度
- 通过修改`train.py`中的参数可以调整训练过程

## 技术细节

- 神经网络使用PyTorch实现，包含卷积层和全连接层
- MCYTS算法基于AlphaGo的核心思想，但规模更小
- 训练过程使用自我对弈和经验回放缓冲区 